# Navigation Examples Using Visual Data

This section provides practical examples of navigation using visual data processed through Isaac ROS VSLAM. These examples demonstrate how to use the maps and localization information generated by visual SLAM systems for autonomous navigation tasks.

## Overview of Visual Navigation

Navigation using visual data involves several key components:
- **Visual Localization**: Determining the robot's pose using visual SLAM
- **Map-Based Planning**: Using the visual map for path planning
- **Visual Servoing**: Direct control using visual features
- **Obstacle Detection**: Identifying and avoiding obstacles using cameras

## Navigation Architecture with VSLAM

### 1. Components of Visual Navigation System

```text
Camera Input → Isaac ROS VSLAM → Map + Pose → Navigation System → Robot Commands
```

The navigation system typically includes:
- **Global Planner**: Path planning using the visual map
- **Local Planner**: Obstacle avoidance using real-time sensor data
- **Controller**: Low-level robot control commands

### 2. Integration with Navigation2

Isaac ROS VSLAM integrates with Navigation2, which provides:
- Action interfaces for navigation goals
- Recovery behaviors for challenging situations
- Costmap management for obstacle avoidance
- Plugin system for custom planners and controllers

## Example 1: Basic Navigation with VSLAM Map

This example demonstrates how to use a map generated by Isaac ROS VSLAM for basic navigation:

```python
#!/usr/bin/env python3
"""
Basic Navigation Example using Isaac ROS VSLAM Map
"""

import rclpy
from rclpy.node import Node
from nav2_msgs.action import NavigateToPose
from geometry_msgs.msg import PoseStamped
from action_msgs.msg import GoalStatus
import time

class VSLAMNavigationExample(Node):
    def __init__(self):
        super().__init__('vslam_navigation_example')
        
        # Create action client for navigation
        self.nav_client = rclpy.action.ActionClient(
            self,
            NavigateToPose,
            'navigate_to_pose'
        )
        
        # Wait for navigation server to be available
        self.nav_client.wait_for_server()
        
        self.get_logger().info('VSLAM Navigation Example Node Initialized')

    def navigate_to_pose(self, x, y, theta):
        """Send navigation goal to move robot to specified pose"""
        
        # Create navigation goal
        goal = NavigateToPose.Goal()
        goal.pose.header.frame_id = 'map'  # Use map frame from VSLAM
        goal.pose.header.stamp = self.get_clock().now().to_msg()
        
        # Set position
        goal.pose.pose.position.x = x
        goal.pose.pose.position.y = y
        goal.pose.pose.position.z = 0.0
        
        # Set orientation (convert theta to quaternion)
        import math
        from tf_transformations import quaternion_from_euler
        
        quat = quaternion_from_euler(0, 0, theta)
        goal.pose.pose.orientation.x = quat[0]
        goal.pose.pose.orientation.y = quat[1]
        goal.pose.pose.orientation.z = quat[2]
        goal.pose.pose.orientation.w = quat[3]
        
        # Send navigation goal
        self.get_logger().info(f'Sending navigation goal: ({x}, {y}, {theta})')
        
        future = self.nav_client.send_goal_async(goal)
        future.add_done_callback(self.goal_response_callback)
        
    def goal_response_callback(self, future):
        """Handle response from navigation server"""
        goal_handle = future.result()
        if not goal_handle.accepted:
            self.get_logger().info('Goal rejected')
            return
            
        self.get_logger().info('Goal accepted')
        result_future = goal_handle.get_result_async()
        result_future.add_done_callback(self.get_result_callback)
        
    def get_result_callback(self, future):
        """Handle navigation result"""
        result = future.result().result
        status = future.result().status
        
        if status == GoalStatus.STATUS_SUCCEEDED:
            self.get_logger().info('Navigation succeeded!')
        else:
            self.get_logger().info(f'Navigation failed with status: {status}')

def main(args=None):
    rclpy.init(args=args)
    
    navigator = VSLAMNavigationExample()
    
    # Example: Navigate to a position using coordinates from VSLAM map
    # These coordinates should be in the map frame established by VSLAM
    navigator.navigate_to_pose(2.0, 1.5, 0.0)  # Move to (2.0, 1.5) facing 0 radians
    
    try:
        rclpy.spin(navigator)
    except KeyboardInterrupt:
        pass
    finally:
        navigator.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Example 2: Visual Obstacle Avoidance

This example shows how to use visual data for obstacle detection and avoidance:

```python
#!/usr/bin/env python3
"""
Visual Obstacle Avoidance Example
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge
import cv2
import numpy as np

class VisualObstacleAvoidance(Node):
    def __init__(self):
        super().__init__('visual_obstacle_avoidance')
        
        # Create subscription to camera image
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_rect_color',
            self.image_callback,
            10
        )
        
        # Create publisher for velocity commands
        self.cmd_vel_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )
        
        # Initialize CV bridge
        self.bridge = CvBridge()
        
        # Timer for control loop
        self.timer = self.create_timer(0.1, self.control_loop)
        
        # State variables
        self.obstacle_detected = False
        self.last_image = None
        self.movement_direction = "forward"
        
        self.get_logger().info('Visual Obstacle Avoidance Node Initialized')

    def image_callback(self, msg):
        """Process incoming camera images to detect obstacles"""
        try:
            # Convert ROS image to OpenCV image
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            self.last_image = cv_image
        except Exception as e:
            self.get_logger().error(f'Error converting image: {e}')

    def detect_obstacles(self, image):
        """Detect obstacles in the image using computer vision"""
        if image is None:
            return False
            
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Apply threshold to detect obstacles (simplified approach)
        # In practice, more sophisticated methods would be used
        _, binary = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)
        
        # Calculate the ratio of non-zero pixels to determine if obstacle exists
        non_zero_pixels = cv2.countNonZero(binary)
        total_pixels = binary.shape[0] * binary.shape[1]
        obstacle_ratio = non_zero_pixels / total_pixels
        
        # If more than 20% of pixels are considered obstacles
        return obstacle_ratio > 0.2

    def control_loop(self):
        """Main control loop for obstacle avoidance"""
        if self.last_image is not None:
            # Detect obstacles in the current image
            self.obstacle_detected = self.detect_obstacles(self.last_image)
            
            # Create velocity command
            cmd_vel = Twist()
            
            if self.obstacle_detected:
                # Obstacle detected - turn
                cmd_vel.linear.x = 0.0
                cmd_vel.angular.z = 0.5  # Turn in place
                self.movement_direction = "turning"
            else:
                # No obstacle - go forward
                cmd_vel.linear.x = 0.5  # Move forward
                cmd_vel.angular.z = 0.0
                self.movement_direction = "forward"
            
            # Publish command
            self.cmd_vel_pub.publish(cmd_vel)
            
            self.get_logger().info(
                f'Obstacle: {self.obstacle_detected}, '
                f'Motion: {self.movement_direction}, '
                f'Linear: {cmd_vel.linear.x}, Angular: {cmd_vel.angular.z}'
            )

def main(args=None):
    rclpy.init(args=args)
    
    obstacle_avoider = VisualObstacleAvoidance()
    
    try:
        rclpy.spin(obstacle_avoider)
    except KeyboardInterrupt:
        pass
    finally:
        obstacle_avoider.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Example 3: Visual Path Following

This example demonstrates following a path using visual features:

```python
#!/usr/bin/env python3
"""
Visual Path Following Example
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from geometry_msgs.msg import Twist
from nav_msgs.msg import Path
from geometry_msgs.msg import PoseStamped
from cv_bridge import CvBridge
import cv2
import numpy as np
from tf2_ros import TransformException
from tf2_ros.buffer import Buffer
from tf2_ros.transform_listener import TransformListener

class VisualPathFollower(Node):
    def __init__(self):
        super().__init__('visual_path_follower')
        
        # Create subscription to camera image
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_rect_color',
            self.image_callback,
            10
        )
        
        # Create publisher for velocity commands
        self.cmd_vel_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )
        
        # Create subscription to path
        self.path_sub = self.create_subscription(
            Path,
            '/plan',  # Path from global planner
            self.path_callback,
            10
        )
        
        # Initialize TF listener to get robot's position
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        
        # Initialize CV bridge
        self.bridge = CvBridge()
        
        # Timer for control loop
        self.timer = self.create_timer(0.05, self.control_loop)  # 20 Hz
        
        # State variables
        self.current_path = []
        self.current_path_index = 0
        self.last_image = None
        self.robot_pose = None
        
        self.get_logger().info('Visual Path Follower Node Initialized')

    def image_callback(self, msg):
        """Process incoming camera images"""
        try:
            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")
            self.last_image = cv_image
        except Exception as e:
            self.get_logger().error(f'Error converting image: {e}')

    def path_callback(self, msg):
        """Receive new path from global planner"""
        self.current_path = [pose.pose.position for pose in msg.poses]
        self.current_path_index = 0
        self.get_logger().info(f'Received new path with {len(self.current_path)} waypoints')

    def get_robot_pose(self):
        """Get robot's current pose using TF"""
        try:
            transform = self.tf_buffer.lookup_transform(
                'map',  # Target frame
                'base_link',  # Source frame
                rclpy.time.Time(),  # Get most recent transform
                rclpy.duration.Duration(seconds=1.0)  # Timeout
            )
            return transform.transform.translation
        except TransformException as e:
            self.get_logger().error(f'Could not get transform: {e}')
            return None

    def detect_path_features(self, image):
        """Detect visual features that help follow the path"""
        if image is None:
            return None
            
        # Convert to HSV for color-based path detection
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        # Define range for a path marker color (e.g., yellow tape)
        lower_color = np.array([20, 100, 100])
        upper_color = np.array([30, 255, 255])
        
        # Create mask for the color
        mask = cv2.inRange(hsv, lower_color, upper_color)
        
        # Find contours of the path markers
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if contours:
            # Find the largest contour
            largest_contour = max(contours, key=cv2.contourArea)
            
            # Calculate centroid of the contour
            M = cv2.moments(largest_contour)
            if M["m00"] != 0:
                cx = int(M["m10"] / M["m00"])
                cy = int(M["m01"] / M["m00"])
                return (cx, cy)
        
        return None

    def control_loop(self):
        """Main control loop for path following"""
        if self.last_image is None or not self.current_path:
            return
        
        # Get robot's current position
        self.robot_pose = self.get_robot_pose()
        if not self.robot_pose:
            return
            
        # Detect visual path features
        path_feature = self.detect_path_features(self.last_image)
        
        # Create velocity command
        cmd_vel = Twist()
        
        if path_feature:
            # Calculate error based on visual path feature position
            image_center_x = self.last_image.shape[1] / 2
            feature_x = path_feature[0]
            
            # Angular control based on lateral error
            lateral_error = feature_x - image_center_x
            angular_correction = 0.002 * lateral_error  # Proportional control
            
            # Linear velocity based on forward progress
            cmd_vel.linear.x = 0.3  # Forward speed
            cmd_vel.angular.z = angular_correction
        else:
            # If no path feature detected, slow down and search
            cmd_vel.linear.x = 0.1
            cmd_vel.angular.z = 0.2  # Slow turn to find path
        
        # Publish command
        self.cmd_vel_pub.publish(cmd_vel)
        
        self.get_logger().info(
            f'Linear: {cmd_vel.linear.x:.3f}, '
            f'Angular: {cmd_vel.angular.z:.3f}'
        )

def main(args=None):
    rclpy.init(args=args)
    
    path_follower = VisualPathFollower()
    
    try:
        rclpy.spin(path_follower)
    except KeyboardInterrupt:
        pass
    finally:
        path_follower.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Example 4: Integration with Isaac ROS VSLAM

This example shows how to integrate with Isaac ROS VSLAM for navigation:

```python
#!/usr/bin/env python3
"""
Isaac ROS VSLAM Integration Example
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import PoseWithCovarianceStamped, Twist
from std_msgs.msg import Bool
from tf2_ros import TransformException
from tf2_ros.buffer import Buffer
from tf2_ros.transform_listener import TransformListener
from visualization_msgs.msg import Marker, MarkerArray
from geometry_msgs.msg import Point
import numpy as np

class IsaacVSLAMNavigator(Node):
    def __init__(self):
        super().__init__('isaac_vslam_navigator')
        
        # Create subscriptions for Isaac ROS VSLAM output
        self.pose_sub = self.create_subscription(
            PoseWithCovarianceStamped,
            '/visual_slam/pose',
            self.pose_callback,
            10
        )
        
        # Create subscription to camera data
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_rect_color',
            self.image_callback,
            10
        )
        
        self.camera_info_sub = self.create_subscription(
            CameraInfo,
            '/camera/camera_info',
            self.camera_info_callback,
            10
        )
        
        # Create publisher for robot commands
        self.cmd_vel_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )
        
        # Publisher for visualization markers
        self.marker_pub = self.create_publisher(
            MarkerArray,
            '/vslam_navigation/markers',
            10
        )
        
        # Initialize TF listener
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        
        # Timer for navigation control
        self.control_timer = self.create_timer(0.05, self.navigation_control)
        
        # State variables
        self.current_pose = None
        self.last_image = None
        self.camera_info = None
        self.navigation_goal = (2.0, 1.0)  # Example goal coordinates
        self.reached_goal = False
        
        self.get_logger().info('Isaac VSLAM Navigator Initialized')

    def pose_callback(self, msg):
        """Receive pose estimates from Isaac ROS VSLAM"""
        self.current_pose = msg.pose.pose
        self.get_logger().info(
            f'VSLAM Pose: ({msg.pose.pose.position.x:.2f}, '
            f'{msg.pose.pose.position.y:.2f})'
        )

    def image_callback(self, msg):
        """Receive camera images"""
        # In a real implementation, we would process images for additional
        # visual navigation cues
        pass

    def camera_info_callback(self, msg):
        """Receive camera calibration information"""
        self.camera_info = msg

    def calculate_navigation_command(self):
        """Calculate velocity commands based on current pose and goal"""
        if not self.current_pose or self.reached_goal:
            return None
            
        # Calculate distance to goal
        pos = self.current_pose.position
        dist_to_goal = np.sqrt(
            (self.navigation_goal[0] - pos.x) ** 2 + 
            (self.navigation_goal[1] - pos.y) ** 2
        )
        
        # Create velocity command
        cmd_vel = Twist()
        
        if dist_to_goal < 0.2:  # 20 cm tolerance
            # Reached goal
            cmd_vel.linear.x = 0.0
            cmd_vel.angular.z = 0.0
            if not self.reached_goal:
                self.get_logger().info('Reached navigation goal!')
                self.reached_goal = True
        else:
            # Calculate direction to goal
            angle_to_goal = np.arctan2(
                self.navigation_goal[1] - pos.y,
                self.navigation_goal[0] - pos.x
            )
            
            # Current orientation
            from tf_transformations import euler_from_quaternion
            orientation_q = [
                self.current_pose.orientation.x,
                self.current_pose.orientation.y,
                self.current_pose.orientation.z,
                self.current_pose.orientation.w
            ]
            _, _, current_yaw = euler_from_quaternion(orientation_q)
            
            # Calculate angular error
            angle_error = angle_to_goal - current_yaw
            
            # Normalize angle to [-π, π]
            while angle_error > np.pi:
                angle_error -= 2 * np.pi
            while angle_error < -np.pi:
                angle_error += 2 * np.pi
                
            # Simple proportional control
            cmd_vel.linear.x = min(0.5, dist_to_goal * 0.5)  # Max 0.5 m/s
            cmd_vel.angular.z = max(-0.5, min(0.5, angle_error * 1.0))
        
        return cmd_vel

    def navigation_control(self):
        """Main navigation control loop"""
        cmd_vel = self.calculate_navigation_command()
        
        if cmd_vel:
            self.cmd_vel_pub.publish(cmd_vel)
            
            pos = self.current_pose.position if self.current_pose else None
            if pos:
                self.get_logger().info(
                    f'Navigating to ({self.navigation_goal[0]}, {self.navigation_goal[1]}) '
                    f'from ({pos.x:.2f}, {pos.y:.2f}), '
                    f'Command: ({cmd_vel.linear.x:.2f}, {cmd_vel.angular.z:.2f})'
                )

    def visualize_navigation(self):
        """Publish visualization markers for navigation path"""
        marker_array = MarkerArray()
        
        # Goal marker
        goal_marker = Marker()
        goal_marker.header.frame_id = "map"
        goal_marker.header.stamp = self.get_clock().now().to_msg()
        goal_marker.ns = "navigation_goals"
        goal_marker.id = 0
        goal_marker.type = Marker.SPHERE
        goal_marker.action = Marker.ADD
        goal_marker.pose.position.x = self.navigation_goal[0]
        goal_marker.pose.position.y = self.navigation_goal[1]
        goal_marker.pose.position.z = 0.0
        goal_marker.pose.orientation.w = 1.0
        goal_marker.scale.x = 0.3
        goal_marker.scale.y = 0.3
        goal_marker.scale.z = 0.3
        goal_marker.color.a = 1.0
        goal_marker.color.r = 0.0
        goal_marker.color.g = 1.0
        goal_marker.color.b = 0.0
        
        marker_array.markers.append(goal_marker)
        
        # Publish the markers
        self.marker_pub.publish(marker_array)

def main(args=None):
    rclpy.init(args=args)
    
    navigator = IsaacVSLAMNavigator()
    
    try:
        rclpy.spin(navigator)
    except KeyboardInterrupt:
        pass
    finally:
        navigator.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()
```

## Launch Configuration

To run these navigation examples with Isaac ROS VSLAM, create a launch file:

```python
# vslam_navigation_launch.py
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument
from launch.substitutions import LaunchConfiguration
from launch_ros.actions import Node
from ament_index_python.packages import get_package_share_directory
import os

def generate_launch_description():
    use_sim_time = LaunchConfiguration('use_sim_time')
    
    # Isaac ROS VSLAM node
    visual_slam_node = Node(
        package='isaac_ros_visual_slam',
        executable='isaac_ros_visual_slam_node',
        name='visual_slam',
        parameters=[{
            'use_sim_time': use_sim_time,
            'enable_occupancy_map': True,
            'acceleration_mode': 'gpu',
        }],
        remappings=[
            ('/stereo_camera/left/image', '/camera/image_left'),
            ('/stereo_camera/right/image', '/camera/image_right'),
            ('/stereo_camera/left/camera_info', '/camera/camera_info_left'),
            ('/stereo_camera/right/camera_info', '/camera/camera_info_right'),
        ]
    )
    
    # Navigation node (using the examples above)
    navigation_node = Node(
        package='your_navigation_package',
        executable='isaac_vslam_navigator',
        name='vslam_navigator',
        parameters=[{
            'use_sim_time': use_sim_time,
        }]
    )
    
    return LaunchDescription([
        DeclareLaunchArgument(
            'use_sim_time',
            default_value='True',
            description='Use simulation clock if true'
        ),
        visual_slam_node,
        navigation_node,
    ])
```

## Visual Navigation Best Practices

### 1. Environment Considerations
- Ensure adequate visual features (texture) for SLAM algorithms
- Avoid repetitive patterns that can confuse feature matching
- Provide sufficient lighting for reliable visual processing

### 2. Sensor Configuration
- Use appropriately calibrated cameras
- Ensure sufficient field of view for navigation tasks
- Consider stereo cameras for enhanced depth perception

### 3. Performance Optimization
- Monitor computational load on the GPU
- Adjust feature density based on available processing power
- Use appropriate image resolution for the task

### 4. Safety Considerations
- Implement fallback navigation strategies
- Include obstacle detection as a safety measure
- Monitor the quality of VSLAM estimates

## Troubleshooting Visual Navigation

### 1. Poor Navigation Performance
- **Symptoms**: Erratic movement, failure to reach goals
- **Solutions**: Check VSLAM map quality, tune controller parameters, verify sensor calibration

### 2. VSLAM Drift Affecting Navigation
- **Symptoms**: Robot position estimate drifting over time
- **Solutions**: Enable loop closure, verify adequate visual features, tune VSLAM parameters

### 3. Feature Depletion in Navigation
- **Symptoms**: Loss of tracking in long visual navigation
- **Solutions**: Plan routes with adequate visual features, implement relocalization

These examples demonstrate various approaches to navigation using visual data processed with Isaac ROS VSLAM, showing how to integrate visual localization with path planning and execution.